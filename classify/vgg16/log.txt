2019-01-12 22:29:12 - INFO - created model with configuration : {'maxEpoches': 300, 'valList': '/home/wfw/data/VOCdevkit/cifar10/test_data.txt', 'trainList': '/home/wfw/data/VOCdevkit/cifar10/train_data.txt', 'test_batch_size': 100, 'train_batch_size': 256}
2019-01-12 22:29:12 - INFO - Train data num: 50000
2019-01-12 22:29:12 - INFO - Test data num: 10000
2019-01-12 22:29:15 - INFO - ------------------------------------------------------------------------------------------------------------------------------------------------------
2019-01-12 22:29:15 - INFO - Layer                           Kernel Shape          Stride        Gradient              Output Shape         # Params (K)      # Mult-Adds (M)
2019-01-12 22:29:15 - INFO - ======================================================================================================================================================
2019-01-12 22:29:15 - INFO - 0_Data          	                   -               -           False          [256, 3, 32, 32]                    _                    _
2019-01-12 22:29:15 - INFO - 1_Conv2d        	       [3, 64, 7, 7]          [2, 2]            True         [256, 64, 16, 16]                 9.41                 2.41
2019-01-12 22:29:15 - INFO - 2_BatchNorm2d   	                [64]               -            True         [256, 64, 16, 16]                 0.13                 0.00
2019-01-12 22:29:15 - INFO - 3_MaxPool2d     	                   -               -               -           [256, 64, 8, 8]                    -                    -
2019-01-12 22:29:15 - INFO - 4_Conv2d        	      [64, 64, 3, 3]          [1, 1]            True           [256, 64, 8, 8]                36.86                 2.36
2019-01-12 22:29:15 - INFO - 5_BatchNorm2d   	                [64]               -            True           [256, 64, 8, 8]                 0.13                 0.00
2019-01-12 22:29:15 - INFO - 6_Conv2d        	      [64, 64, 3, 3]          [1, 1]            True           [256, 64, 8, 8]                36.86                 2.36
2019-01-12 22:29:15 - INFO - 7_BatchNorm2d   	                [64]               -            True           [256, 64, 8, 8]                 0.13                 0.00
2019-01-12 22:29:15 - INFO - 8_Sequential    	                   -               -               -           [256, 64, 8, 8]                    -                    -
2019-01-12 22:29:15 - INFO - 9_Conv2d        	      [64, 64, 3, 3]          [1, 1]            True           [256, 64, 8, 8]                36.86                 2.36
2019-01-12 22:29:15 - INFO - 10_BatchNorm2d  	                [64]               -            True           [256, 64, 8, 8]                 0.13                 0.00
2019-01-12 22:29:15 - INFO - 11_Conv2d       	      [64, 64, 3, 3]          [1, 1]            True           [256, 64, 8, 8]                36.86                 2.36
2019-01-12 22:29:15 - INFO - 12_BatchNorm2d  	                [64]               -            True           [256, 64, 8, 8]                 0.13                 0.00
2019-01-12 22:29:15 - INFO - 13_Sequential   	                   -               -               -           [256, 64, 8, 8]                    -                    -
2019-01-12 22:29:15 - INFO - 14_Conv2d       	     [64, 128, 3, 3]          [2, 2]            True          [256, 128, 4, 4]                73.73                 1.18
2019-01-12 22:29:15 - INFO - 15_BatchNorm2d  	               [128]               -            True          [256, 128, 4, 4]                 0.26                 0.00
2019-01-12 22:29:15 - INFO - 16_Conv2d       	    [128, 128, 3, 3]          [1, 1]            True          [256, 128, 4, 4]               147.46                 2.36
2019-01-12 22:29:15 - INFO - 17_BatchNorm2d  	               [128]               -            True          [256, 128, 4, 4]                 0.26                 0.00
2019-01-12 22:29:15 - INFO - 18_Conv2d       	     [64, 128, 1, 1]          [2, 2]            True          [256, 128, 4, 4]                 8.19                 0.13
2019-01-12 22:29:15 - INFO - 19_BatchNorm2d  	               [128]               -            True          [256, 128, 4, 4]                 0.26                 0.00
2019-01-12 22:29:15 - INFO - 20_Conv2d       	    [128, 128, 3, 3]          [1, 1]            True          [256, 128, 4, 4]               147.46                 2.36
2019-01-12 22:29:15 - INFO - 21_BatchNorm2d  	               [128]               -            True          [256, 128, 4, 4]                 0.26                 0.00
2019-01-12 22:29:15 - INFO - 22_Conv2d       	    [128, 128, 3, 3]          [1, 1]            True          [256, 128, 4, 4]               147.46                 2.36
2019-01-12 22:29:15 - INFO - 23_BatchNorm2d  	               [128]               -            True          [256, 128, 4, 4]                 0.26                 0.00
2019-01-12 22:29:15 - INFO - 24_Sequential   	                   -               -               -          [256, 128, 4, 4]                    -                    -
2019-01-12 22:29:15 - INFO - 25_Conv2d       	    [128, 256, 3, 3]          [2, 2]            True          [256, 256, 2, 2]               294.91                 1.18
2019-01-12 22:29:15 - INFO - 26_BatchNorm2d  	               [256]               -            True          [256, 256, 2, 2]                 0.51                 0.00
2019-01-12 22:29:15 - INFO - 27_Conv2d       	    [256, 256, 3, 3]          [1, 1]            True          [256, 256, 2, 2]               589.82                 2.36
2019-01-12 22:29:15 - INFO - 28_BatchNorm2d  	               [256]               -            True          [256, 256, 2, 2]                 0.51                 0.00
2019-01-12 22:29:15 - INFO - 29_Conv2d       	    [128, 256, 1, 1]          [2, 2]            True          [256, 256, 2, 2]                32.77                 0.13
2019-01-12 22:29:15 - INFO - 30_BatchNorm2d  	               [256]               -            True          [256, 256, 2, 2]                 0.51                 0.00
2019-01-12 22:29:15 - INFO - 31_Conv2d       	    [256, 256, 3, 3]          [1, 1]            True          [256, 256, 2, 2]               589.82                 2.36
2019-01-12 22:29:15 - INFO - 32_BatchNorm2d  	               [256]               -            True          [256, 256, 2, 2]                 0.51                 0.00
2019-01-12 22:29:15 - INFO - 33_Conv2d       	    [256, 256, 3, 3]          [1, 1]            True          [256, 256, 2, 2]               589.82                 2.36
2019-01-12 22:29:15 - INFO - 34_BatchNorm2d  	               [256]               -            True          [256, 256, 2, 2]                 0.51                 0.00
2019-01-12 22:29:15 - INFO - 35_Sequential   	                   -               -               -          [256, 256, 2, 2]                    -                    -
2019-01-12 22:29:15 - INFO - 36_Conv2d       	    [256, 512, 3, 3]          [2, 2]            True          [256, 512, 1, 1]             1,179.65                 1.18
2019-01-12 22:29:15 - INFO - 37_BatchNorm2d  	               [512]               -            True          [256, 512, 1, 1]                 1.02                 0.00
2019-01-12 22:29:15 - INFO - 38_Conv2d       	    [512, 512, 3, 3]          [1, 1]            True          [256, 512, 1, 1]             2,359.30                 2.36
2019-01-12 22:29:15 - INFO - 39_BatchNorm2d  	               [512]               -            True          [256, 512, 1, 1]                 1.02                 0.00
2019-01-12 22:29:15 - INFO - 40_Conv2d       	    [256, 512, 1, 1]          [2, 2]            True          [256, 512, 1, 1]               131.07                 0.13
2019-01-12 22:29:15 - INFO - 41_BatchNorm2d  	               [512]               -            True          [256, 512, 1, 1]                 1.02                 0.00
2019-01-12 22:29:15 - INFO - 42_Conv2d       	    [512, 512, 3, 3]          [1, 1]            True          [256, 512, 1, 1]             2,359.30                 2.36
2019-01-12 22:29:15 - INFO - 43_BatchNorm2d  	               [512]               -            True          [256, 512, 1, 1]                 1.02                 0.00
2019-01-12 22:29:15 - INFO - 44_Conv2d       	    [512, 512, 3, 3]          [1, 1]            True          [256, 512, 1, 1]             2,359.30                 2.36
2019-01-12 22:29:15 - INFO - 45_BatchNorm2d  	               [512]               -            True          [256, 512, 1, 1]                 1.02                 0.00
2019-01-12 22:29:15 - INFO - 46_Sequential   	                   -               -               -          [256, 512, 1, 1]                    -                    -
2019-01-12 22:29:15 - INFO - 47_AdaptiveAvgPool2d 	                   -               -               -          [256, 512, 1, 1]                    -                    -
2019-01-12 22:29:15 - INFO - 48_Linear       	           [512, 10]               -            True                 [256, 10]                 5.13                 0.01
2019-01-12 22:29:15 - INFO - ======================================================================================================================================================
2019-01-12 22:29:15 - INFO - # Params:    11,181.64K
2019-01-12 22:29:15 - INFO - # Mult-Adds: 37.02M
2019-01-12 22:29:15 - INFO - # GFLOPS:    0.0000G
2019-01-12 22:29:15 - INFO - ------------------------------------------------------------------------------------------------------------------------------------------------------
2019-01-12 22:29:15 - INFO - OPTIMIZER - setting method = SGD
2019-01-12 22:29:15 - INFO - OPTIMIZER - group 0 setting momentum = 0.9
2019-01-12 22:29:15 - INFO - OPTIMIZER - group 0 setting lr = 0.001
2019-01-12 22:29:15 - INFO - OPTIMIZER - group 0 setting weight_decay = 0.0005
2019-01-12 22:29:15 - INFO - TRAINING - Epoch: [0][0/196]	Time 0.464 (0.464)	Data 0.086 (0.086)	LR: [0.0010000/NULL]	Loss 2.5357 (2.5357)	Prec@1 11.328 (11.328)	Prec@5 49.219 (49.219)
2019-01-12 22:29:16 - INFO - TRAINING - Epoch: [0][20/196]	Time 0.051 (0.067)	Data 0.045 (0.042)	LR: [0.0010000/NULL]	Loss 2.2375 (2.3534)	Prec@1 16.797 (12.984)	Prec@5 62.891 (56.120)
2019-01-12 22:29:17 - INFO - TRAINING - Epoch: [0][40/196]	Time 0.050 (0.059)	Data 0.044 (0.044)	LR: [0.0010000/NULL]	Loss 2.1062 (2.2636)	Prec@1 23.047 (16.244)	Prec@5 75.000 (63.681)
2019-01-12 22:29:18 - INFO - TRAINING - Epoch: [0][60/196]	Time 0.051 (0.057)	Data 0.045 (0.044)	LR: [0.0010000/NULL]	Loss 1.9116 (2.1844)	Prec@1 28.906 (19.262)	Prec@5 80.859 (68.231)
2019-01-12 22:29:19 - INFO - TRAINING - Epoch: [0][80/196]	Time 0.050 (0.055)	Data 0.044 (0.044)	LR: [0.0010000/NULL]	Loss 1.9109 (2.1300)	Prec@1 32.812 (21.340)	Prec@5 82.031 (71.079)
2019-01-12 22:29:20 - INFO - TRAINING - Epoch: [0][100/196]	Time 0.051 (0.054)	Data 0.045 (0.044)	LR: [0.0010000/NULL]	Loss 1.9772 (2.0820)	Prec@1 29.297 (23.252)	Prec@5 78.906 (73.345)
2019-01-12 22:29:21 - INFO - TRAINING - Epoch: [0][120/196]	Time 0.051 (0.054)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.8591 (2.0406)	Prec@1 33.203 (24.848)	Prec@5 82.812 (75.065)
2019-01-12 22:29:22 - INFO - TRAINING - Epoch: [0][140/196]	Time 0.051 (0.053)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.6942 (2.0070)	Prec@1 33.203 (26.078)	Prec@5 90.234 (76.593)
2019-01-12 22:29:23 - INFO - TRAINING - Epoch: [0][160/196]	Time 0.050 (0.053)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.7740 (1.9787)	Prec@1 33.594 (27.084)	Prec@5 85.938 (77.710)
2019-01-12 22:29:24 - INFO - TRAINING - Epoch: [0][180/196]	Time 0.051 (0.053)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.6906 (1.9519)	Prec@1 38.672 (28.039)	Prec@5 87.109 (78.649)
2019-01-12 22:29:25 - INFO - EVALUATING - Epoch: [0][0/100]	Time 0.149 (0.149)	Data 0.074 (0.074)	LR: [NULL/NULL]	Loss 1.6973 (1.6973)	Prec@1 37.000 (37.000)	Prec@5 86.000 (86.000)
2019-01-12 22:29:26 - INFO - EVALUATING - Epoch: [0][20/100]	Time 0.020 (0.026)	Data 0.017 (0.018)	LR: [NULL/NULL]	Loss 1.7760 (1.7111)	Prec@1 33.000 (37.000)	Prec@5 87.000 (87.952)
2019-01-12 22:29:26 - INFO - EVALUATING - Epoch: [0][40/100]	Time 0.020 (0.023)	Data 0.018 (0.018)	LR: [NULL/NULL]	Loss 1.8493 (1.6992)	Prec@1 35.000 (36.707)	Prec@5 83.000 (87.951)
2019-01-12 22:29:27 - INFO - EVALUATING - Epoch: [0][60/100]	Time 0.020 (0.022)	Data 0.017 (0.018)	LR: [NULL/NULL]	Loss 1.5443 (1.7060)	Prec@1 42.000 (36.869)	Prec@5 93.000 (87.672)
2019-01-12 22:29:27 - INFO - EVALUATING - Epoch: [0][80/100]	Time 0.020 (0.021)	Data 0.018 (0.018)	LR: [NULL/NULL]	Loss 1.6284 (1.6920)	Prec@1 31.000 (37.099)	Prec@5 93.000 (87.963)
2019-01-12 22:29:28 - INFO - 
 Epoch: 1	Training Loss 1.9360 	Training Prec@1 28.552 	Training Prec@5 79.222 	Validation Loss 1.6983 	Validation Prec@1 37.000 	Validation Prec@5 87.850 

2019-01-12 22:29:28 - INFO - OPTIMIZER - setting method = SGD
2019-01-12 22:29:28 - INFO - OPTIMIZER - group 0 setting momentum = 0.9
2019-01-12 22:29:28 - INFO - OPTIMIZER - group 0 setting weight_decay = 0.0005
2019-01-12 22:29:28 - INFO - TRAINING - Epoch: [1][0/196]	Time 0.122 (0.122)	Data 0.113 (0.113)	LR: [0.0010000/NULL]	Loss 1.6504 (1.6504)	Prec@1 39.844 (39.844)	Prec@5 88.281 (88.281)
2019-01-12 22:29:29 - INFO - TRAINING - Epoch: [1][20/196]	Time 0.050 (0.054)	Data 0.044 (0.048)	LR: [0.0010000/NULL]	Loss 1.6852 (1.6853)	Prec@1 35.156 (36.942)	Prec@5 87.109 (88.039)
2019-01-12 22:29:30 - INFO - TRAINING - Epoch: [1][40/196]	Time 0.050 (0.052)	Data 0.044 (0.046)	LR: [0.0010000/NULL]	Loss 1.6919 (1.6708)	Prec@1 38.672 (37.853)	Prec@5 87.500 (88.157)
2019-01-12 22:29:31 - INFO - TRAINING - Epoch: [1][60/196]	Time 0.051 (0.052)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.7141 (1.6592)	Prec@1 35.938 (38.480)	Prec@5 85.938 (88.358)
2019-01-12 22:29:32 - INFO - TRAINING - Epoch: [1][80/196]	Time 0.051 (0.052)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.6134 (1.6546)	Prec@1 40.625 (38.754)	Prec@5 89.062 (88.489)
2019-01-12 22:29:33 - INFO - TRAINING - Epoch: [1][100/196]	Time 0.051 (0.052)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.5837 (1.6530)	Prec@1 42.578 (38.788)	Prec@5 90.234 (88.521)
2019-01-12 22:29:34 - INFO - TRAINING - Epoch: [1][120/196]	Time 0.051 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.5210 (1.6440)	Prec@1 45.703 (39.208)	Prec@5 90.234 (88.572)
2019-01-12 22:29:35 - INFO - TRAINING - Epoch: [1][140/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.5448 (1.6367)	Prec@1 41.406 (39.547)	Prec@5 92.188 (88.691)
2019-01-12 22:29:36 - INFO - TRAINING - Epoch: [1][160/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.5099 (1.6301)	Prec@1 42.969 (39.778)	Prec@5 93.359 (88.847)
2019-01-12 22:29:37 - INFO - TRAINING - Epoch: [1][180/196]	Time 0.051 (0.051)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.4686 (1.6260)	Prec@1 51.562 (39.937)	Prec@5 89.453 (88.920)
2019-01-12 22:29:38 - INFO - EVALUATING - Epoch: [1][0/100]	Time 0.050 (0.050)	Data 0.047 (0.047)	LR: [NULL/NULL]	Loss 1.4766 (1.4766)	Prec@1 42.000 (42.000)	Prec@5 94.000 (94.000)
2019-01-12 22:29:38 - INFO - EVALUATING - Epoch: [1][20/100]	Time 0.022 (0.022)	Data 0.019 (0.020)	LR: [NULL/NULL]	Loss 1.5829 (1.5625)	Prec@1 39.000 (41.048)	Prec@5 90.000 (91.333)
2019-01-12 22:29:39 - INFO - EVALUATING - Epoch: [1][40/100]	Time 0.020 (0.021)	Data 0.018 (0.019)	LR: [NULL/NULL]	Loss 1.6214 (1.5473)	Prec@1 46.000 (42.366)	Prec@5 89.000 (91.146)
2019-01-12 22:29:39 - INFO - EVALUATING - Epoch: [1][60/100]	Time 0.020 (0.021)	Data 0.017 (0.018)	LR: [NULL/NULL]	Loss 1.4449 (1.5524)	Prec@1 44.000 (42.377)	Prec@5 93.000 (90.869)
2019-01-12 22:29:39 - INFO - EVALUATING - Epoch: [1][80/100]	Time 0.023 (0.021)	Data 0.020 (0.019)	LR: [NULL/NULL]	Loss 1.4915 (1.5409)	Prec@1 48.000 (42.716)	Prec@5 85.000 (91.074)
2019-01-12 22:29:40 - INFO - 
 Epoch: 2	Training Loss 1.6220 	Training Prec@1 40.128 	Training Prec@5 89.066 	Validation Loss 1.5508 	Validation Prec@1 42.550 	Validation Prec@5 90.710 

2019-01-12 22:29:40 - INFO - OPTIMIZER - setting method = SGD
2019-01-12 22:29:40 - INFO - OPTIMIZER - group 0 setting momentum = 0.9
2019-01-12 22:29:40 - INFO - OPTIMIZER - group 0 setting weight_decay = 0.0005
2019-01-12 22:29:40 - INFO - TRAINING - Epoch: [2][0/196]	Time 0.089 (0.089)	Data 0.080 (0.080)	LR: [0.0010000/NULL]	Loss 1.5398 (1.5398)	Prec@1 41.797 (41.797)	Prec@5 89.062 (89.062)
2019-01-12 22:29:41 - INFO - TRAINING - Epoch: [2][20/196]	Time 0.050 (0.053)	Data 0.044 (0.046)	LR: [0.0010000/NULL]	Loss 1.4886 (1.5385)	Prec@1 43.750 (43.843)	Prec@5 89.453 (90.327)
2019-01-12 22:29:42 - INFO - TRAINING - Epoch: [2][40/196]	Time 0.051 (0.052)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.5715 (1.5388)	Prec@1 44.531 (43.531)	Prec@5 87.891 (90.511)
2019-01-12 22:29:43 - INFO - TRAINING - Epoch: [2][60/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.4704 (1.5286)	Prec@1 43.750 (43.814)	Prec@5 94.141 (90.862)
2019-01-12 22:29:44 - INFO - TRAINING - Epoch: [2][80/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.4887 (1.5276)	Prec@1 44.922 (43.986)	Prec@5 94.141 (90.842)
2019-01-12 22:29:45 - INFO - TRAINING - Epoch: [2][100/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.5834 (1.5238)	Prec@1 41.797 (44.106)	Prec@5 89.844 (90.927)
2019-01-12 22:29:46 - INFO - TRAINING - Epoch: [2][120/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.5614 (1.5264)	Prec@1 41.797 (43.944)	Prec@5 89.844 (90.961)
2019-01-12 22:29:47 - INFO - TRAINING - Epoch: [2][140/196]	Time 0.050 (0.051)	Data 0.044 (0.044)	LR: [0.0010000/NULL]	Loss 1.4624 (1.5199)	Prec@1 49.609 (44.335)	Prec@5 91.797 (91.032)
2019-01-12 22:29:48 - INFO - TRAINING - Epoch: [2][160/196]	Time 0.051 (0.051)	Data 0.044 (0.044)	LR: [0.0010000/NULL]	Loss 1.5301 (1.5142)	Prec@1 47.266 (44.461)	Prec@5 90.625 (91.127)
2019-01-12 22:29:49 - INFO - TRAINING - Epoch: [2][180/196]	Time 0.051 (0.051)	Data 0.044 (0.044)	LR: [0.0010000/NULL]	Loss 1.4991 (1.5093)	Prec@1 48.828 (44.695)	Prec@5 92.188 (91.206)
2019-01-12 22:29:50 - INFO - EVALUATING - Epoch: [2][0/100]	Time 0.050 (0.050)	Data 0.046 (0.046)	LR: [NULL/NULL]	Loss 1.4274 (1.4274)	Prec@1 45.000 (45.000)	Prec@5 93.000 (93.000)
2019-01-12 22:29:51 - INFO - EVALUATING - Epoch: [2][20/100]	Time 0.020 (0.021)	Data 0.017 (0.019)	LR: [NULL/NULL]	Loss 1.4783 (1.4697)	Prec@1 48.000 (45.429)	Prec@5 95.000 (92.810)
2019-01-12 22:29:51 - INFO - EVALUATING - Epoch: [2][40/100]	Time 0.020 (0.021)	Data 0.017 (0.018)	LR: [NULL/NULL]	Loss 1.5376 (1.4623)	Prec@1 43.000 (46.024)	Prec@5 92.000 (92.488)
2019-01-12 22:29:51 - INFO - EVALUATING - Epoch: [2][60/100]	Time 0.020 (0.020)	Data 0.017 (0.018)	LR: [NULL/NULL]	Loss 1.3329 (1.4637)	Prec@1 53.000 (46.541)	Prec@5 96.000 (92.344)
2019-01-12 22:29:52 - INFO - EVALUATING - Epoch: [2][80/100]	Time 0.020 (0.020)	Data 0.017 (0.018)	LR: [NULL/NULL]	Loss 1.4335 (1.4534)	Prec@1 48.000 (46.358)	Prec@5 90.000 (92.383)
2019-01-12 22:29:52 - INFO - 
 Epoch: 3	Training Loss 1.5065 	Training Prec@1 44.804 	Training Prec@5 91.266 	Validation Loss 1.4638 	Validation Prec@1 46.340 	Validation Prec@5 91.920 

2019-01-12 22:29:52 - INFO - OPTIMIZER - setting method = SGD
2019-01-12 22:29:52 - INFO - OPTIMIZER - group 0 setting momentum = 0.9
2019-01-12 22:29:52 - INFO - OPTIMIZER - group 0 setting weight_decay = 0.0005
2019-01-12 22:29:52 - INFO - TRAINING - Epoch: [3][0/196]	Time 0.086 (0.086)	Data 0.079 (0.079)	LR: [0.0010000/NULL]	Loss 1.4640 (1.4640)	Prec@1 47.266 (47.266)	Prec@5 92.578 (92.578)
2019-01-12 22:29:53 - INFO - TRAINING - Epoch: [3][20/196]	Time 0.051 (0.052)	Data 0.045 (0.046)	LR: [0.0010000/NULL]	Loss 1.3820 (1.4386)	Prec@1 46.094 (46.838)	Prec@5 93.359 (92.336)
2019-01-12 22:29:54 - INFO - TRAINING - Epoch: [3][40/196]	Time 0.051 (0.052)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.3806 (1.4492)	Prec@1 46.875 (47.123)	Prec@5 91.797 (92.159)
2019-01-12 22:29:55 - INFO - TRAINING - Epoch: [3][60/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.4803 (1.4449)	Prec@1 47.266 (47.067)	Prec@5 89.453 (92.207)
2019-01-12 22:29:56 - INFO - TRAINING - Epoch: [3][80/196]	Time 0.053 (0.051)	Data 0.047 (0.045)	LR: [0.0010000/NULL]	Loss 1.4304 (1.4431)	Prec@1 45.312 (47.323)	Prec@5 92.969 (92.192)
2019-01-12 22:29:57 - INFO - TRAINING - Epoch: [3][100/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.4029 (1.4420)	Prec@1 51.562 (47.401)	Prec@5 92.969 (92.222)
2019-01-12 22:29:58 - INFO - TRAINING - Epoch: [3][120/196]	Time 0.051 (0.051)	Data 0.045 (0.045)	LR: [0.0010000/NULL]	Loss 1.4275 (1.4405)	Prec@1 47.656 (47.369)	Prec@5 91.797 (92.317)
2019-01-12 22:29:59 - INFO - TRAINING - Epoch: [3][140/196]	Time 0.050 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.4153 (1.4399)	Prec@1 46.484 (47.440)	Prec@5 92.969 (92.318)
2019-01-12 22:30:00 - INFO - TRAINING - Epoch: [3][160/196]	Time 0.052 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.3720 (1.4353)	Prec@1 49.219 (47.656)	Prec@5 94.141 (92.367)
2019-01-12 22:30:01 - INFO - TRAINING - Epoch: [3][180/196]	Time 0.052 (0.051)	Data 0.044 (0.045)	LR: [0.0010000/NULL]	Loss 1.4283 (1.4309)	Prec@1 47.656 (47.846)	Prec@5 91.797 (92.397)
